{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "cBWZ7ZjLmCCH"
      },
      "outputs": [],
      "source": [
        "# -------------------------\n",
        "# üì¶ 1. Import Dependencies\n",
        "# -------------------------\n",
        "import numpy as np\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------------------\n",
        "# üåê 2. Define Maze Environment\n",
        "# -------------------------------\n",
        "# 4x4 grid. Agent starts at (0, 0), Goal at (3, 3)\n",
        "\n",
        "grid_size = 4\n",
        "start = (0, 0)\n",
        "goal = (3, 3)\n",
        "walls = []  # optional obstacles\n",
        "\n",
        "# All valid positions (no walls in this teaching model)\n",
        "def is_valid(state):\n",
        "    x, y = state\n",
        "    return 0 <= x < grid_size and 0 <= y < grid_size and state not in walls"
      ],
      "metadata": {
        "id": "k7FuS7a-mRpp"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------------------------\n",
        "# üìä 3. Initialize Q-Table and Params\n",
        "# ----------------------------------\n",
        "\n",
        "actions = ['up', 'down', 'left', 'right']\n",
        "q_table = {}\n",
        "\n",
        "# Initialize Q-values for all valid states and actions\n",
        "for x in range(grid_size):\n",
        "    for y in range(grid_size):\n",
        "        if is_valid((x, y)):\n",
        "            q_table[(x, y)] = {a: 0.0 for a in actions}\n",
        "\n",
        "# Learning parameters\n",
        "alpha = 0.7        # Learning rate\n",
        "gamma = 0.9        # Discount factor\n",
        "epsilon = 0.2      # Exploration rate\n",
        "episodes = 200     # Number of training runs"
      ],
      "metadata": {
        "id": "DwDERIzlmcJf"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------------------------------\n",
        "# üéØ 4. Reward Function and Next State Logic\n",
        "# ----------------------------------------\n",
        "\n",
        "def move(state, action):\n",
        "    x, y = state\n",
        "    if action == 'up': x -= 1\n",
        "    elif action == 'down': x += 1\n",
        "    elif action == 'left': y -= 1\n",
        "    elif action == 'right': y += 1\n",
        "\n",
        "    next_state = (x, y)\n",
        "    if not is_valid(next_state):\n",
        "        return state, -1  # invalid move = penalty\n",
        "    if next_state == goal:\n",
        "        return next_state, 10  # reward at goal\n",
        "    return next_state, -0.1  # small penalty per move"
      ],
      "metadata": {
        "id": "VcUdN9EMmm2o"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------------------------\n",
        "# ü§ñ 5. Q-Learning Training Loop Begins\n",
        "# -------------------------------------\n",
        "\n",
        "for ep in range(episodes):\n",
        "    state = start\n",
        "    while state != goal:\n",
        "        if random.random() < epsilon:\n",
        "            action = random.choice(actions)  # Explore\n",
        "        else:\n",
        "            action = max(q_table[state], key=q_table[state].get)  # Exploit best\n",
        "\n",
        "        next_state, reward = move(state, action)\n",
        "\n",
        "        # Q-learning formula\n",
        "        old_value = q_table[state][action]\n",
        "        next_max = max(q_table[next_state].values())\n",
        "\n",
        "        q_table[state][action] = old_value + alpha * (reward + gamma * next_max - old_value)\n",
        "\n",
        "        state = next_state"
      ],
      "metadata": {
        "id": "GrMaoh7Dm4U3"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------------------------\n",
        "# üìà 6. Visualize Learned Q-Table\n",
        "# ----------------------------------\n",
        "\n",
        "# Optional: print the best action per state\n",
        "policy_grid = np.full((grid_size, grid_size), '', dtype=object)\n",
        "\n",
        "for (x, y), actions_dict in q_table.items():\n",
        "    best_action = max(actions_dict, key=actions_dict.get)\n",
        "    policy_grid[x][y] = best_action[0].upper()\n",
        "\n",
        "policy_grid[goal] = 'üèÅ'\n",
        "\n",
        "print(\"Policy Grid (Best Actions):\")\n",
        "print(policy_grid)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cn0JpM1-m-g8",
        "outputId": "1629ba09-e4b8-4ab4-a013-ed7a2d2074b1"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Policy Grid (Best Actions):\n",
            "[['R' 'D' 'L' 'D']\n",
            " ['R' 'D' 'D' 'D']\n",
            " ['R' 'R' 'R' 'D']\n",
            " ['R' 'U' 'R' 'üèÅ']]\n"
          ]
        }
      ]
    }
  ]
}