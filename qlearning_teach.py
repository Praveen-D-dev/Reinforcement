# -*- coding: utf-8 -*-
"""qlearning_teach.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1zI33xhF4uWMP0Yl8kCd1g_1ismPmnQQg
"""

# -------------------------
# üì¶ 1. Import Dependencies
# -------------------------
import numpy as np
import random
import matplotlib.pyplot as plt
import seaborn as sns

# -------------------------------
# üåê 2. Define Maze Environment
# -------------------------------
# 4x4 grid. Agent starts at (0, 0), Goal at (3, 3)

grid_size = 4
start = (0, 0)
goal = (3, 3)
walls = []  # optional obstacles

# All valid positions (no walls in this teaching model)
def is_valid(state):
    x, y = state
    return 0 <= x < grid_size and 0 <= y < grid_size and state not in walls

# ----------------------------------
# üìä 3. Initialize Q-Table and Params
# ----------------------------------

actions = ['up', 'down', 'left', 'right']
q_table = {}

# Initialize Q-values for all valid states and actions
for x in range(grid_size):
    for y in range(grid_size):
        if is_valid((x, y)):
            q_table[(x, y)] = {a: 0.0 for a in actions}

# Learning parameters
alpha = 0.7        # Learning rate
gamma = 0.9        # Discount factor
epsilon = 0.2      # Exploration rate
episodes = 200     # Number of training runs

# ----------------------------------------
# üéØ 4. Reward Function and Next State Logic
# ----------------------------------------

def move(state, action):
    x, y = state
    if action == 'up': x -= 1
    elif action == 'down': x += 1
    elif action == 'left': y -= 1
    elif action == 'right': y += 1

    next_state = (x, y)
    if not is_valid(next_state):
        return state, -1  # invalid move = penalty
    if next_state == goal:
        return next_state, 10  # reward at goal
    return next_state, -0.1  # small penalty per move

# -------------------------------------
# ü§ñ 5. Q-Learning Training Loop Begins
# -------------------------------------

for ep in range(episodes):
    state = start
    while state != goal:
        if random.random() < epsilon:
            action = random.choice(actions)  # Explore
        else:
            action = max(q_table[state], key=q_table[state].get)  # Exploit best

        next_state, reward = move(state, action)

        # Q-learning formula
        old_value = q_table[state][action]
        next_max = max(q_table[next_state].values())

        q_table[state][action] = old_value + alpha * (reward + gamma * next_max - old_value)

        state = next_state

# ----------------------------------
# üìà 6. Visualize Learned Q-Table
# ----------------------------------

# Optional: print the best action per state
policy_grid = np.full((grid_size, grid_size), '', dtype=object)

for (x, y), actions_dict in q_table.items():
    best_action = max(actions_dict, key=actions_dict.get)
    policy_grid[x][y] = best_action[0].upper()

policy_grid[goal] = 'üèÅ'

print("Policy Grid (Best Actions):")
print(policy_grid)